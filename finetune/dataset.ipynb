{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a dataset of reddit conversations for finetuning a GPT-style language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-26 00:42:33.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoaded 54215 rows from data/processed/posts-11-13-2024-processed.json\u001b[0m\n",
      "\u001b[32m2024-11-26 00:42:34.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mConverted json to pandas DataFrame with 54215 rows\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>downs</th>\n",
       "      <th>name</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>is_video</th>\n",
       "      <th>deleted</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1dx1b0z</td>\n",
       "      <td>Destiny</td>\n",
       "      <td></td>\n",
       "      <td>New Vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_1dx1b0z</td>\n",
       "      <td>0.95</td>\n",
       "      <td>121</td>\n",
       "      <td>None</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1720304607</td>\n",
       "      <td>None</td>\n",
       "      <td>TuningsGaming</td>\n",
       "      <td>2</td>\n",
       "      <td>248289</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'id': 'lbyv8mn', 'total_awards_received': 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id subreddit selftext      title  downs        name  upvote_ratio  \\\n",
       "0  1dx1b0z   Destiny           New Vegan      0  t3_1dx1b0z          0.95   \n",
       "\n",
       "   ups removed_by_category link_flair_text  ...  no_follow  created_utc  \\\n",
       "0  121                None        Shitpost  ...      False   1720304607   \n",
       "\n",
       "  author_flair_text         author num_comments  subreddit_subscribers  \\\n",
       "0              None  TuningsGaming            2                 248289   \n",
       "\n",
       "   send_replies is_video deleted  \\\n",
       "0          True    False   False   \n",
       "\n",
       "                                            comments  \n",
       "0  [{'id': 'lbyv8mn', 'total_awards_received': 0,...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import PROCESSED_DATA_FILE, HUGGINGFACE_USERNAME\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "with open(PROCESSED_DATA_FILE, 'r') as f:\n",
    "  data = json.load(f)\n",
    "logger.info(f\"Loaded {len(data)} rows from {PROCESSED_DATA_FILE}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "prefilter_len = len(df)\n",
    "logger.info(f\"Converted json to pandas DataFrame with {prefilter_len} rows\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['destiny' 'hasan_piker' 'politics' 'vaushv' 'millenials' 'news'\n",
      " 'worldnews' 'economics' 'socialism' 'conservative' 'libertarian'\n",
      " 'neoliberal' 'republican' 'democrats' 'progressive' 'daverubin'\n",
      " 'jordanpeterson' 'samharris' 'joerogan' 'thedavidpakmanshow' 'benshapiro'\n",
      " 'themajorityreport' 'seculartalk']\n"
     ]
    }
   ],
   "source": [
    "# alter columns so they're easier to work with\n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "\n",
    "# Show some values that might be helpful for customizing configuration\n",
    "print(df['subreddit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-26 00:42:34.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1msubreddits remaining: ['destiny' 'hasan_piker' 'politics' 'vaushv' 'millenials' 'news'\n",
      " 'worldnews' 'economics' 'socialism' 'conservative' 'libertarian'\n",
      " 'neoliberal' 'republican' 'democrats' 'progressive' 'daverubin'\n",
      " 'jordanpeterson' 'samharris' 'joerogan' 'thedavidpakmanshow' 'benshapiro'\n",
      " 'themajorityreport' 'seculartalk']\u001b[0m\n",
      "\u001b[32m2024-11-26 00:42:34.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mFiltered out 0 rows\u001b[0m\n",
      "\u001b[32m2024-11-26 00:42:34.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mRemaining rows: 54215\u001b[0m\n",
      "\u001b[32m2024-11-26 00:42:34.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m54k rows\u001b[0m\n",
      "\u001b[32m2024-11-26 00:42:34.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mUsing dataset size: 54k\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from config import SUBREDDITS\n",
    "\n",
    "# COMMENT THIS OUT TO USE ALL SUBREDDITS\n",
    "# df = df[df['subreddit'].isin([sub.lower() for sub in SUBREDDITS])]\n",
    "\n",
    "logger.info(f\"subreddits remaining: {df['subreddit'].unique()}\")\n",
    "logger.info(f\"Filtered out {prefilter_len - len(df)} rows\")\n",
    "logger.info(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "from utils import to_k, get_conversations_file\n",
    "posts_count = to_k(len(df), logger)\n",
    "logger.info(f\"Using dataset size: {posts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through posts and create conversations by alternating user/assistant with every comment/reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Generator, List\n",
    "def Turn(role: str, value: str) -> Dict[str, str]:\n",
    "  return {\n",
    "    'from': role,\n",
    "    'value': value\n",
    "  }\n",
    "\n",
    "def traverse_thread(comment: Dict, role: str = 'gpt') -> Generator[List[Dict[str, str]], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively traverse a comment thread and yield each individual thread.\n",
    "    \"\"\"\n",
    "    if role not in {'gpt', 'human'}:\n",
    "        raise ValueError(\"role must be 'gpt' or 'human'\")\n",
    "    \n",
    "    if not comment.get('body'):\n",
    "        return\n",
    "    \n",
    "    if comment['body'] == '[deleted]' or comment['body'] == '[removed]':\n",
    "        return\n",
    "    \n",
    "    # Start the thread with the current comment\n",
    "    current_thread = [Turn(role, comment['body'])]\n",
    "    \n",
    "    # If no replies, yield the current thread as-is\n",
    "    if not comment.get('replies'):\n",
    "        yield current_thread\n",
    "        return\n",
    "    \n",
    "    # Recurse into replies, yielding a full thread for each reply chain\n",
    "    for reply in comment['replies']:\n",
    "        for sub_thread in traverse_thread(reply, 'human' if role == 'gpt' else 'gpt'):\n",
    "            yield current_thread + sub_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-26 00:42:45.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mExtracted 642940 conversations from 54215 posts\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'from': 'system',\n",
       "  'value': 'You are a redditor on r/democrats and you are having a conversation with another redditor.'},\n",
       " {'from': 'human', 'value': 'Honestly disappointed for our country'},\n",
       " {'from': 'gpt',\n",
       "  'value': 'When all is said and done, donuts to dollars the young failed us again by not voting. Unbelievable.'},\n",
       " {'from': 'human',\n",
       "  'value': 'Dems have to give people candidates that they can believe in. Biden shouldâ€™ve dropped out way sooner so we couldâ€™ve had a primary. Couple the economy with not having a primary and you get a candidate that people are not inspired by. Kind of hard to believe in and have enthusiasm for a candidate when youâ€™re worrying about being able to afford to eat or have a place to sleep.'},\n",
       " {'from': 'gpt',\n",
       "  'value': \"Even so, most older Gen Z and Young Millenials remember very well the shitshow Trump caused from 2017-2021. \\n\\nIt should be enough to not vote for him regardless of the other candidate's qualities at this point.\"},\n",
       " {'from': 'human',\n",
       "  'value': 'Do we want to operate on \"should be\" or reality?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import is_post_valid\n",
    "\n",
    "# set to prevent duplicates which can occur if the final comment is deleted or removed\n",
    "conversations = set()\n",
    "for i, post_row in df.iterrows():\n",
    "    system_turn = Turn('system', f\"You are a redditor on r/{post_row['subreddit']} and you are having a conversation with another redditor.\")\n",
    "    valid, reason = is_post_valid(post_row)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    if post_row['selftext'] == '[deleted]' or post_row['selftext'] == '[removed]':\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, 'human'):\n",
    "                # Serialize thread for hashable set element\n",
    "                serialized_thread = json.dumps(thread)\n",
    "                conversations.add(serialized_thread)\n",
    "    else:\n",
    "        initial_turn = Turn('human', post_row['selftext'] if post_row['selftext'] else post_row['title'])\n",
    "        \n",
    "        # Process comments\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment):\n",
    "                # Serialize thread for hashable set element\n",
    "                serialized_thread = json.dumps([system_turn] + [initial_turn] + thread)\n",
    "                conversations.add(serialized_thread)\n",
    "\n",
    "# Deserialize conversations back into Python objects if needed\n",
    "conversations = [json.loads(conv) for conv in conversations]\n",
    "logger.info(f\"Extracted {len(conversations)} conversations from {len(df)} posts\")\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this and more ways to judge conversations\n",
    "# def judge_str_toxicity(text: str) -> float:\n",
    "#     \"\"\"\n",
    "#     Judge the toxicity of a string.\n",
    "#     \"\"\"\n",
    "#     from transformers import pipeline\n",
    "#     classifier = pipeline('text-classification', model='persiainbert/toxic-mahyar', tokenizer='persiainbert/toxic-mahyar')\n",
    "#     result = classifier(text)[0]\n",
    "#     return result['score']\n",
    "\n",
    "# def judge_convo_toxicity(convo: List[Dict[str, str]]) -> float:\n",
    "#     \"\"\"\n",
    "#     Judge the toxicity of a conversation.\n",
    "#     \"\"\"\n",
    "#     return sum(judge_str_toxicity(turn['value']) for turn in convo) / len(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-26 00:42:45.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m642k rows\u001b[0m\n",
      "\u001b[32m2024-11-26 00:42:45.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mSaving 642k conversations to file\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save to jsonl file\n",
    "import json\n",
    "subreddits_str = '-'.join(SUBREDDITS)\n",
    "size_str = to_k(len(conversations), logger)\n",
    "logger.info(f\"Saving {size_str} conversations to file\")\n",
    "conversations_file, name = get_conversations_file(subreddits_str, size_str)\n",
    "\n",
    "json_obj = []\n",
    "for i, conversation in enumerate(conversations):\n",
    "  json_obj.append({\n",
    "    \"conversation\": conversation,\n",
    "    # TODO: add more fields\n",
    "    # \"toxicity_rating\": judge_convo_toxicity(conversation), # TODO: implement this\n",
    "  })\n",
    "with open(conversations_file, 'w') as f:\n",
    "  json.dump(json_obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1e3c6e3a65416a873e318e2de3ffda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# push to huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=conversations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4118849b56cf49729180773e7e34e715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1065d910fb542e681d39a8d53e8a278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/643 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('HF_TOKEN'):\n",
    "  logger.error(\"No Hugging Face token found, not pushing to hub\")\n",
    "else:\n",
    "  dataset.push_to_hub(f\"{HUGGINGFACE_USERNAME}/{name}\".lower(), token=os.getenv('HF_TOKEN'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
