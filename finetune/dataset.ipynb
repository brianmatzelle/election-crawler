{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a base dataset of reddit conversations, so that it can be filtered and subset in the future.\n",
    "\n",
    "### To Do\n",
    "- Add custom metadata based on analysis of the conversation\n",
    "  - [X] normalized controversiality\n",
    "  - [ ] fix normalized contr., currently some values are greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "# Description: Configuration for the dataset module, could eventually be used as flags\n",
    "HUGGINGFACE_USERNAME = 'brianmatzelle'\n",
    "\n",
    "# change if you know what you're doing\n",
    "RAW_DATA_FILE_NAME = 'posts-11-13-2024'\n",
    "\n",
    "# DONT CHANGE\n",
    "RAW_DATA_FILE = f'data/raw/{RAW_DATA_FILE_NAME}.json'\n",
    "PROCESSED_DATA_FILE = f'data/processed/{RAW_DATA_FILE_NAME}-processed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 12:18:12.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLoaded 54215 rows from data/processed/posts-11-13-2024-processed.json\u001b[0m\n",
      "\u001b[32m2024-11-27 12:18:12.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mConverted json to pandas DataFrame with 54215 rows\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>downs</th>\n",
       "      <th>name</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>is_video</th>\n",
       "      <th>deleted</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1dx1b0z</td>\n",
       "      <td>Destiny</td>\n",
       "      <td></td>\n",
       "      <td>New Vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_1dx1b0z</td>\n",
       "      <td>0.95</td>\n",
       "      <td>121</td>\n",
       "      <td>None</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1720304607</td>\n",
       "      <td>None</td>\n",
       "      <td>TuningsGaming</td>\n",
       "      <td>2</td>\n",
       "      <td>248289</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'id': 'lbyv8mn', 'total_awards_received': 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id subreddit selftext      title  downs        name  upvote_ratio  \\\n",
       "0  1dx1b0z   Destiny           New Vegan      0  t3_1dx1b0z          0.95   \n",
       "\n",
       "   ups removed_by_category link_flair_text  ...  no_follow  created_utc  \\\n",
       "0  121                None        Shitpost  ...      False   1720304607   \n",
       "\n",
       "  author_flair_text         author num_comments  subreddit_subscribers  \\\n",
       "0              None  TuningsGaming            2                 248289   \n",
       "\n",
       "   send_replies is_video deleted  \\\n",
       "0          True    False   False   \n",
       "\n",
       "                                            comments  \n",
       "0  [{'id': 'lbyv8mn', 'total_awards_received': 0,...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "with open(PROCESSED_DATA_FILE, 'r') as f:\n",
    "  data = json.load(f)\n",
    "logger.info(f\"Loaded {len(data)} rows from {PROCESSED_DATA_FILE}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "prefilter_len = len(df)\n",
    "logger.info(f\"Converted json to pandas DataFrame with {prefilter_len} rows\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 12:18:12.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m54k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 12:18:12.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mDataset size (posts): 54k\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['destiny' 'hasan_piker' 'politics' 'vaushv' 'millenials' 'news'\n",
      " 'worldnews' 'economics' 'socialism' 'conservative' 'libertarian'\n",
      " 'neoliberal' 'republican' 'democrats' 'progressive' 'daverubin'\n",
      " 'jordanpeterson' 'samharris' 'joerogan' 'thedavidpakmanshow' 'benshapiro'\n",
      " 'themajorityreport' 'seculartalk']\n"
     ]
    }
   ],
   "source": [
    "# alter columns so they're easier to work with\n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "\n",
    "# Show some values that might be helpful for customizing configuration\n",
    "print(df['subreddit'].unique())\n",
    "\n",
    "from utils import to_k\n",
    "posts_count = to_k(len(df))\n",
    "logger.info(f\"Dataset size (posts): {posts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through posts and create conversations by alternating user/assistant with every comment/reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Generator, List\n",
    "def Turn(role: str, content: str) -> Dict[str, str]:\n",
    "  return {\n",
    "    'role': role,\n",
    "    'content': content\n",
    "  }\n",
    "\n",
    "def traverse_thread(comment: Dict, controversiality_sum: Dict, role: str = 'assistant') -> Generator[List[Dict[str, str]], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively traverse a comment thread and yield each individual thread.\n",
    "    \"\"\"\n",
    "    if role not in {'assistant', 'user'}:\n",
    "        raise ValueError(\"role must be 'assistant' or 'user'\")\n",
    "    \n",
    "    if not comment.get('body'):\n",
    "        return\n",
    "    \n",
    "    if comment['body'] == '[deleted]' or comment['body'] == '[removed]':\n",
    "        return\n",
    "  \n",
    "    controversiality_sum['val'] += comment.get('controversiality')\n",
    "    \n",
    "    # Start the thread with the current comment\n",
    "    current_thread = [Turn(role, comment['body'])]\n",
    "    \n",
    "    # If no replies, yield the current thread as-is\n",
    "    if not comment.get('replies'):\n",
    "        yield current_thread\n",
    "        return\n",
    "    \n",
    "    # Recurse into replies, yielding a full thread for each reply chain\n",
    "    for reply in comment['replies']:\n",
    "        for sub_thread in traverse_thread(reply, controversiality_sum, 'user' if role == 'assistant' else 'assistant'):\n",
    "            yield current_thread + sub_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- Add custom metadata based on analysis of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 12:18:28.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mExtracted 643685 conversations from 54215 posts\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metadata': {'subreddit': {'name': 'politics', 'subscribers': 8599504},\n",
       "  'post': {'score': 924,\n",
       "   'upvotes': 924,\n",
       "   'downvotes': 0,\n",
       "   'upvote_ratio': 0.98,\n",
       "   'flair': None,\n",
       "   'author': 'PandaMuffin1',\n",
       "   'suggested_sort': None},\n",
       "  'controversiality': 0,\n",
       "  'normalized_controversiality': 0.0},\n",
       " 'conversation': [{'role': 'system',\n",
       "   'content': 'You are a redditor, having a conversation with another redditor.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Harris leads Trump in Arizona, Gallego holds 11-point lead over Lake: Survey'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"I am hoping for a blue wave across the board in Arizona. In addition to the presidency and the senate race, keep in mind that a blue wave would flip both chambers of the AZ state legislature. Right now they are controlled by the republicans by very slim margins (31-29 in the House, and 16-14 in the senate). \\n\\nVote blue, my friends, and let's make it happen\\n\\nhttps://azdem.org/\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'There is also two Republican-held house seats in Biden won districts as well (AZ-1 and AZ-6) that could flip the chamber.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'I just moved out of CD-1 back to Gallegoâ€™s solid blue district. Bummed I canâ€™t help flip that seat. Itâ€™s supposed to be the most vulnerable of the red seats.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import is_post_valid\n",
    "from lib.analysis import normalize_controversiality_rating\n",
    "# set to prevent duplicates which can occur if the final comment is deleted or removed\n",
    "conversations = set()\n",
    "for i, post_row in df.iterrows():\n",
    "    valid, reason = is_post_valid(post_row)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        \"subreddit\": {\n",
    "            \"name\": post_row.get(\"subreddit\", \"unknown\"),\n",
    "            \"subscribers\": post_row.get(\"subreddit_subscribers\", None),\n",
    "        },\n",
    "        \"post\": {\n",
    "            \"score\": post_row.get(\"score\", None),\n",
    "            \"upvotes\": post_row.get(\"ups\", None),\n",
    "            \"downvotes\": post_row.get(\"downs\", None),\n",
    "            \"upvote_ratio\": post_row.get(\"upvote_ratio\", None),\n",
    "            \"flair\": post_row.get(\"link_flair_text\", None),\n",
    "            \"author\": post_row.get(\"author\", \"unknown\"),\n",
    "            \"suggested_sort\": post_row.get(\"suggested_sort\", None),\n",
    "        },\n",
    "        \"controversiality\": 0,\n",
    "        \"normalized_controversiality\": 0\n",
    "    }\n",
    "\n",
    "    controversiality_sum = {\"val\": 0}\n",
    "    system_turn = Turn('system', f\"You are a redditor, having a conversation with another redditor.\")\n",
    "\n",
    "    # if the post is deleted or removed, use the first comment as the initial turn (user)\n",
    "    if post_row['selftext'] == '[deleted]' or post_row['selftext'] == '[removed]':\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, controversiality_sum, 'user'):\n",
    "                # set controversiality metadata\n",
    "                metadata[\"controversiality\"] = controversiality_sum[\"val\"]\n",
    "                metadata[\"normalized_controversiality\"] = normalize_controversiality_rating(sum=controversiality_sum[\"val\"], thread_length=len(thread))\n",
    "                # Serialize thread with metadata\n",
    "                serialized_thread = json.dumps({\n",
    "                    \"metadata\": metadata,\n",
    "                    \"conversation\": [system_turn] + thread\n",
    "                })\n",
    "                conversations.add(serialized_thread)\n",
    "    # otherwise, use the post title or selftext as the initial turn (user)\n",
    "    else:\n",
    "        initial_turn = Turn('user', post_row['selftext'] if post_row['selftext'] else post_row['title'])\n",
    "        # Process comments\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, controversiality_sum):\n",
    "                # prepend the initial turn to the thread, since the post is not deleted or removed\n",
    "                thread = [initial_turn] + thread\n",
    "                # set controversiality metadata\n",
    "                metadata[\"controversiality\"] = controversiality_sum[\"val\"]\n",
    "                metadata[\"normalized_controversiality\"] = normalize_controversiality_rating(sum=controversiality_sum[\"val\"], thread_length=len(thread)-1) # subtract 1 since post does not have a controversiality rating\n",
    "                # Serialize thread with metadata\n",
    "                serialized_thread = json.dumps({\n",
    "                    \"metadata\": metadata,\n",
    "                    \"conversation\": [system_turn] + thread\n",
    "                })\n",
    "                conversations.add(serialized_thread)\n",
    "\n",
    "# Deserialize conversations back into Python objects if needed\n",
    "conversations = [json.loads(conv) for conv in conversations]\n",
    "logger.info(f\"Extracted {len(conversations)} conversations from {len(df)} posts\")\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates, not sure if they're the reason why controversiality normalization is off (sometimes is higher than 1)\n",
    "# import json\n",
    "# from typing import List, Dict\n",
    "\n",
    "# def scan_duplicates(conversations: List[Dict]) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Scan conversations for duplicates based on the conversation content.\n",
    "#     \"\"\"\n",
    "#     # Use a set for fast lookup of seen conversation contents\n",
    "#     seen_contents = set()\n",
    "#     duplicates = []\n",
    "\n",
    "#     for conversation in conversations:\n",
    "#         # Serialize the conversation content\n",
    "#         conversation_content = json.dumps(conversation[\"conversation\"], sort_keys=True)\n",
    "        \n",
    "#         # Check if the conversation content is already seen\n",
    "#         if conversation_content in seen_contents:\n",
    "#             duplicates.append(conversation)\n",
    "#         else:\n",
    "#             seen_contents.add(conversation_content)\n",
    "\n",
    "#     return duplicates\n",
    "\n",
    "# # Scan for duplicates and log results\n",
    "# duplicate_conversations = scan_duplicates(conversations)\n",
    "# logger.info(f\"Found {len(duplicate_conversations)} duplicate conversations\")\n",
    "\n",
    "# # Save duplicate conversations for further analysis\n",
    "# duplicate_conversations_path = f'data/datasets/{name}-duplicate-conversations.json'\n",
    "# with open(duplicate_conversations_path, 'w') as f:\n",
    "#     json.dump(duplicate_conversations, f, indent=2)\n",
    "\n",
    "# logger.info(f\"Duplicate conversations saved to {duplicate_conversations_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 12:18:28.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m643k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 12:18:28.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mSaving 643k conversations to file\u001b[0m\n",
      "\u001b[32m2024-11-27 12:18:46.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mConversations saved to data/datasets/2024-election-subreddit-threads-643k(1).json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON file\n",
    "import json\n",
    "\n",
    "name = '2024-election-subreddit-threads'\n",
    "size_str = to_k(len(conversations))\n",
    "logger.info(f\"Saving {size_str} conversations to file\")\n",
    "\n",
    "from utils import make_dataset_path\n",
    "dataset_path, hf_name = make_dataset_path(name, size_str)\n",
    "\n",
    "# Create JSON object with metadata and conversation\n",
    "json_obj = []\n",
    "for conversation_data in conversations:\n",
    "    # Each conversation_data should already include metadata and conversation structure\n",
    "    json_obj.append({\n",
    "        \"metadata\": conversation_data.get(\"metadata\", {}),\n",
    "        \"conversations\": conversation_data.get(\"conversation\", [])\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(json_obj, f, indent=2)\n",
    "logger.info(f\"Conversations saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228c1ec5c5ab46f0b1bfa711d63b598c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399c0429154f4b75bea22ad0eee90672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827109fbca2c450dbf39324b4f448615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/322 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6fdbc77209473697b5b714e8f45e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/322 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249cae954c4e449c9795e96ed39dcefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# push to huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=dataset_path)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('HF_TOKEN'):\n",
    "  logger.error(\"No Hugging Face token found, not pushing to hub\")\n",
    "else:\n",
    "  dataset.push_to_hub(f\"{HUGGINGFACE_USERNAME}/{hf_name}\".lower(), token=os.getenv('HF_TOKEN'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
