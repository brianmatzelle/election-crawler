{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PROCESSED_DATA_FILE\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "with open(PROCESSED_DATA_FILE, 'r') as f:\n",
    "  data = json.load(f)\n",
    "logger.info(f\"Loaded {len(data)} rows from {PROCESSED_DATA_FILE}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "prefilter_len = len(df)\n",
    "logger.info(f\"Converted json to pandas DataFrame with {prefilter_len} rows\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter columns so they're easier to work with\n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "\n",
    "# Show some values that might be helpful for customizing configuration\n",
    "print(df['subreddit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SUBREDDITS\n",
    "df = df[df['subreddit'].isin([sub.lower() for sub in SUBREDDITS])]\n",
    "\n",
    "logger.info(f\"subreddits remaining: {df['subreddit'].unique()}\")\n",
    "logger.info(f\"Filtered out {prefilter_len - len(df)} rows\")\n",
    "logger.info(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "from utils import to_k, get_conversations_file\n",
    "posts_count = to_k(len(df), logger)\n",
    "logger.info(f\"Using dataset size: {posts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through posts and create conversations by alternating user/assistant with every comment/reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Generator, List\n",
    "def Turn(role: str, value: str) -> Dict[str, str]:\n",
    "  return {\n",
    "    'from': role,\n",
    "    'value': value\n",
    "  }\n",
    "\n",
    "def traverse_thread(comment: Dict, role: str = 'gpt') -> Generator[List[Dict[str, str]], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively traverse a comment thread and yield each individual thread.\n",
    "    \"\"\"\n",
    "    if role not in {'gpt', 'human'}:\n",
    "        raise ValueError(\"role must be 'gpt' or 'human'\")\n",
    "    \n",
    "    if not comment.get('body'):\n",
    "        return\n",
    "    \n",
    "    if comment['body'] == '[deleted]' or comment['body'] == '[removed]':\n",
    "        return\n",
    "    \n",
    "    # Start the thread with the current comment\n",
    "    current_thread = [Turn(role, comment['body'])]\n",
    "    \n",
    "    # If no replies, yield the current thread as-is\n",
    "    if not comment.get('replies'):\n",
    "        yield current_thread\n",
    "        return\n",
    "    \n",
    "    # Recurse into replies, yielding a full thread for each reply chain\n",
    "    for reply in comment['replies']:\n",
    "        for sub_thread in traverse_thread(reply, 'human' if role == 'gpt' else 'gpt'):\n",
    "            yield current_thread + sub_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_post_valid\n",
    "conversations = []\n",
    "for i, post_row in df.iterrows():\n",
    "    valid, reason = is_post_valid(post_row)\n",
    "    if not valid:\n",
    "      # logger.debug(f\"Skipping post {post_row['id']} because it is not valid: {reason}\")\n",
    "      continue\n",
    "\n",
    "    if post_row['selftext'] == '[deleted]' or post_row['selftext'] == '[removed]':\n",
    "        # logger.warning(f\"Starting thread with first comment because post {post_row['id']} is deleted or removed: {post_row['title']}\")\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, 'human'):\n",
    "                conversations.append(thread)\n",
    "    else:\n",
    "      initial_turn = Turn('human', post_row['selftext'] if post_row['selftext'] else post_row['title'])\n",
    "      \n",
    "      # Process comments\n",
    "      for comment in post_row.get('comments', []):\n",
    "          for thread in traverse_thread(comment):\n",
    "              conversations.append([initial_turn] + thread)\n",
    "\n",
    "conversations[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to jsonl file\n",
    "import json\n",
    "subreddits_str = '-'.join(SUBREDDITS)\n",
    "size_str = to_k(len(conversations), logger)\n",
    "logger.info(f\"Saving {size_str} conversations to file\")\n",
    "conversations_file = get_conversations_file(subreddits_str, size_str)\n",
    "with open(conversations_file, 'w') as f:\n",
    "    for conv in conversations:\n",
    "        f.write(json.dumps(conv) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
