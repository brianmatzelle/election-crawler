{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PROCESSED_DATA_FILE, HUGGINGFACE_USERNAME\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "with open(PROCESSED_DATA_FILE, 'r') as f:\n",
    "  data = json.load(f)\n",
    "logger.info(f\"Loaded {len(data)} rows from {PROCESSED_DATA_FILE}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "prefilter_len = len(df)\n",
    "logger.info(f\"Converted json to pandas DataFrame with {prefilter_len} rows\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter columns so they're easier to work with\n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "\n",
    "# Show some values that might be helpful for customizing configuration\n",
    "print(df['subreddit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SUBREDDITS\n",
    "\n",
    "# COMMENT THIS OUT TO USE ALL SUBREDDITS\n",
    "# df = df[df['subreddit'].isin([sub.lower() for sub in SUBREDDITS])]\n",
    "\n",
    "logger.info(f\"subreddits remaining: {df['subreddit'].unique()}\")\n",
    "logger.info(f\"Filtered out {prefilter_len - len(df)} rows\")\n",
    "logger.info(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "from utils import to_k, get_conversations_file\n",
    "posts_count = to_k(len(df), logger)\n",
    "logger.info(f\"Using dataset size: {posts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through posts and create conversations by alternating user/assistant with every comment/reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Generator, List\n",
    "def Turn(role: str, value: str) -> Dict[str, str]:\n",
    "  return {\n",
    "    'from': role,\n",
    "    'value': value\n",
    "  }\n",
    "\n",
    "def traverse_thread(comment: Dict, role: str = 'gpt') -> Generator[List[Dict[str, str]], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively traverse a comment thread and yield each individual thread.\n",
    "    \"\"\"\n",
    "    if role not in {'gpt', 'human'}:\n",
    "        raise ValueError(\"role must be 'gpt' or 'human'\")\n",
    "    \n",
    "    if not comment.get('body'):\n",
    "        return\n",
    "    \n",
    "    if comment['body'] == '[deleted]' or comment['body'] == '[removed]':\n",
    "        return\n",
    "    \n",
    "    # Start the thread with the current comment\n",
    "    current_thread = [Turn(role, comment['body'])]\n",
    "    \n",
    "    # If no replies, yield the current thread as-is\n",
    "    if not comment.get('replies'):\n",
    "        yield current_thread\n",
    "        return\n",
    "    \n",
    "    # Recurse into replies, yielding a full thread for each reply chain\n",
    "    for reply in comment['replies']:\n",
    "        for sub_thread in traverse_thread(reply, 'human' if role == 'gpt' else 'gpt'):\n",
    "            yield current_thread + sub_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_post_valid\n",
    "\n",
    "# set to prevent duplicates which can occur if the final comment is deleted or removed\n",
    "conversations = set()\n",
    "for i, post_row in df.iterrows():\n",
    "    system_turn = Turn('system', f\"You are a redditor on r/{post_row['subreddit']} and you are having a conversation with another redditor.\")\n",
    "    valid, reason = is_post_valid(post_row)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    if post_row['selftext'] == '[deleted]' or post_row['selftext'] == '[removed]':\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, 'human'):\n",
    "                # Serialize thread for hashable set element\n",
    "                serialized_thread = json.dumps(thread)\n",
    "                conversations.add(serialized_thread)\n",
    "    else:\n",
    "        initial_turn = Turn('human', post_row['selftext'] if post_row['selftext'] else post_row['title'])\n",
    "        \n",
    "        # Process comments\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment):\n",
    "                # Serialize thread for hashable set element\n",
    "                serialized_thread = json.dumps([system_turn] + [initial_turn] + thread)\n",
    "                conversations.add(serialized_thread)\n",
    "\n",
    "# Deserialize conversations back into Python objects if needed\n",
    "conversations = [json.loads(conv) for conv in conversations]\n",
    "logger.info(f\"Extracted {len(conversations)} conversations from {len(df)} posts\")\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this and more ways to judge conversations\n",
    "# def judge_str_toxicity(text: str) -> float:\n",
    "#     \"\"\"\n",
    "#     Judge the toxicity of a string.\n",
    "#     \"\"\"\n",
    "#     from transformers import pipeline\n",
    "#     classifier = pipeline('text-classification', model='persiainbert/toxic-mahyar', tokenizer='persiainbert/toxic-mahyar')\n",
    "#     result = classifier(text)[0]\n",
    "#     return result['score']\n",
    "\n",
    "# def judge_convo_toxicity(convo: List[Dict[str, str]]) -> float:\n",
    "#     \"\"\"\n",
    "#     Judge the toxicity of a conversation.\n",
    "#     \"\"\"\n",
    "#     return sum(judge_str_toxicity(turn['value']) for turn in convo) / len(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to jsonl file\n",
    "import json\n",
    "subreddits_str = '-'.join(SUBREDDITS)\n",
    "size_str = to_k(len(conversations), logger)\n",
    "logger.info(f\"Saving {size_str} conversations to file\")\n",
    "conversations_file, name = get_conversations_file(subreddits_str, size_str)\n",
    "\n",
    "json_obj = []\n",
    "for i, conversation in enumerate(conversations):\n",
    "  json_obj.append({\n",
    "    \"conversation\": conversation,\n",
    "    # TODO: add more fields\n",
    "    # \"toxicity_rating\": judge_convo_toxicity(conversation), # TODO: implement this\n",
    "  })\n",
    "with open(conversations_file, 'w') as f:\n",
    "  json.dump(json_obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=conversations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('HF_TOKEN'):\n",
    "  logger.error(\"No Hugging Face token found, not pushing to hub\")\n",
    "else:\n",
    "  dataset.push_to_hub(f\"{HUGGINGFACE_USERNAME}/{name}\".lower(), token=os.getenv('HF_TOKEN'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
