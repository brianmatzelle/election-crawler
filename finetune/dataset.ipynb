{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a base dataset of reddit conversations, so that it can be filtered and subset in the future.\n",
    "\n",
    "### To Do\n",
    "- Add custom metadata based on analysis of the conversation\n",
    "  - [X] normalized controversiality\n",
    "  - [ ] fix normalized contr., currently some values are greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "# Description: Configuration for the dataset module, could eventually be used as flags\n",
    "HUGGINGFACE_USERNAME = 'brianmatzelle'\n",
    "\n",
    "# change if you know what you're doing\n",
    "RAW_DATA_FILE_NAME = 'posts-11-13-2024'\n",
    "\n",
    "# DONT CHANGE\n",
    "RAW_DATA_FILE = f'data/raw/{RAW_DATA_FILE_NAME}.json'\n",
    "PROCESSED_DATA_FILE = f'data/processed/{RAW_DATA_FILE_NAME}-processed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "with open(PROCESSED_DATA_FILE, 'r') as f:\n",
    "  data = json.load(f)\n",
    "logger.info(f\"Loaded {len(data)} rows from {PROCESSED_DATA_FILE}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "prefilter_len = len(df)\n",
    "logger.info(f\"Converted json to pandas DataFrame with {prefilter_len} rows\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter columns so they're easier to work with\n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "\n",
    "# Show some values that might be helpful for customizing configuration\n",
    "print(df['subreddit'].unique())\n",
    "\n",
    "from utils import to_k\n",
    "posts_count = to_k(len(df))\n",
    "logger.info(f\"Dataset size (posts): {posts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through posts and create conversations by alternating user/assistant with every comment/reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Generator, List\n",
    "def Turn(role: str, content: str) -> Dict[str, str]:\n",
    "  return {\n",
    "    'role': role,\n",
    "    'content': content\n",
    "  }\n",
    "\n",
    "def traverse_thread(comment: Dict, controversiality_sum: Dict, role: str = 'assistant') -> Generator[List[Dict[str, str]], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively traverse a comment thread and yield each individual thread.\n",
    "    \"\"\"\n",
    "    if role not in {'assistant', 'user'}:\n",
    "        raise ValueError(\"role must be 'assistant' or 'user'\")\n",
    "    \n",
    "    if not comment.get('body'):\n",
    "        return\n",
    "    \n",
    "    if comment['body'] == '[deleted]' or comment['body'] == '[removed]':\n",
    "        return\n",
    "  \n",
    "    controversiality_sum['val'] += comment.get('controversiality')\n",
    "    \n",
    "    # Start the thread with the current comment\n",
    "    current_thread = [Turn(role, comment['body'])]\n",
    "    \n",
    "    # If no replies, yield the current thread as-is\n",
    "    if not comment.get('replies'):\n",
    "        yield current_thread\n",
    "        return\n",
    "    \n",
    "    # Recurse into replies, yielding a full thread for each reply chain\n",
    "    for reply in comment['replies']:\n",
    "        for sub_thread in traverse_thread(reply, controversiality_sum, 'user' if role == 'assistant' else 'assistant'):\n",
    "            yield current_thread + sub_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- Add custom metadata based on analysis of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(post_row):\n",
    "  return {\n",
    "    \"subreddit\": {\n",
    "      \"name\": post_row.get(\"subreddit\", \"unknown\"),\n",
    "      \"subscribers\": post_row.get(\"subreddit_subscribers\", None),\n",
    "    },\n",
    "    \"post\": {\n",
    "      \"score\": post_row.get(\"score\", None),\n",
    "      \"upvotes\": post_row.get(\"ups\", None),\n",
    "      \"downvotes\": post_row.get(\"downs\", None),\n",
    "      \"upvote_ratio\": post_row.get(\"upvote_ratio\", None),\n",
    "      \"flair\": post_row.get(\"link_flair_text\", None),\n",
    "      \"author\": post_row.get(\"author\", \"unknown\"),\n",
    "      \"suggested_sort\": post_row.get(\"suggested_sort\", None),\n",
    "    },\n",
    "    \"controversiality\": 0,\n",
    "    \"normalized_controversiality\": 0\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_post_valid\n",
    "from lib.analysis import normalize_controversiality_rating\n",
    "\n",
    "# set to prevent duplicates which can occur if the final comment is deleted or removed\n",
    "conversations = set()\n",
    "for i, post_row in df.iterrows():\n",
    "    valid, reason = is_post_valid(post_row)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = get_metadata(post_row)\n",
    "\n",
    "    system_turn = Turn('system', f\"You are a redditor, having a conversation with another redditor.\")\n",
    "\n",
    "    # if the post is deleted or removed, use the first comment as the initial turn (user)\n",
    "    if post_row['selftext'] == '[deleted]' or post_row['selftext'] == '[removed]':\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, controversiality_sum, 'user'):\n",
    "                controversiality_sum = {\"val\": 0}\n",
    "                # set controversiality metadata\n",
    "                metadata[\"controversiality\"] = controversiality_sum[\"val\"]\n",
    "                metadata[\"normalized_controversiality\"] = normalize_controversiality_rating(sum=controversiality_sum[\"val\"], thread_length=len(thread))\n",
    "                # Serialize thread with metadata\n",
    "                serialized_thread = json.dumps({\n",
    "                    \"metadata\": metadata,\n",
    "                    \"conversation\": [system_turn] + thread\n",
    "                })\n",
    "                conversations.add(serialized_thread)\n",
    "    # otherwise, use the post title or selftext as the initial turn (user)\n",
    "    else:\n",
    "        initial_turn = Turn('user', post_row['selftext'] if post_row['selftext'] else post_row['title'])\n",
    "        # Process comments\n",
    "        for comment in post_row.get('comments', []):\n",
    "            for thread in traverse_thread(comment, controversiality_sum):\n",
    "                controversiality_sum = {\"val\": 0}\n",
    "                # prepend the initial turn to the thread, since the post is not deleted or removed\n",
    "                thread = [initial_turn] + thread\n",
    "                # set controversiality metadata\n",
    "                metadata[\"controversiality\"] = controversiality_sum[\"val\"]\n",
    "                metadata[\"normalized_controversiality\"] = normalize_controversiality_rating(sum=controversiality_sum[\"val\"], thread_length=len(thread)-1) # subtract 1 since post does not have a controversiality rating\n",
    "                # Serialize thread with metadata\n",
    "                serialized_thread = json.dumps({\n",
    "                    \"metadata\": metadata,\n",
    "                    \"conversation\": [system_turn] + thread\n",
    "                })\n",
    "                conversations.add(serialized_thread)\n",
    "\n",
    "\n",
    "# Deserialize conversations back into Python objects if needed\n",
    "conversations = [json.loads(conv) for conv in conversations]\n",
    "logger.info(f\"Extracted {len(conversations)} conversations from {len(df)} posts\")\n",
    "logger.info(f\"Deleting dataframe from memory since it hoards resources and is no longer needed\")\n",
    "del df\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "import json\n",
    "\n",
    "name = '2024-election-subreddit-threads'\n",
    "size_str = to_k(len(conversations))\n",
    "from utils import make_dataset_path\n",
    "dataset_path, hf_dataset_name = make_dataset_path(name, size_str)\n",
    "logger.info(f\"Writing {size_str} conversations to {dataset_path}...\")\n",
    "\n",
    "\n",
    "# Create JSON object with metadata and conversation\n",
    "json_obj = []\n",
    "for conversation_data in conversations:\n",
    "    # Each conversation_data should already include metadata and conversation structure\n",
    "    json_obj.append({\n",
    "        \"metadata\": conversation_data.get(\"metadata\", {}),\n",
    "        \"conversations\": conversation_data.get(\"conversation\", [])\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(json_obj, f, indent=2)\n",
    "logger.info(f\"Conversations saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=dataset_path)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('HF_TOKEN'):\n",
    "  logger.error(\"No Hugging Face token found, not pushing to hub\")\n",
    "else:\n",
    "  dataset.push_to_hub(f\"{HUGGINGFACE_USERNAME}/{hf_dataset_name}\".lower(), token=os.getenv('HF_TOKEN'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (election_env)",
   "language": "python",
   "name": "election_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
