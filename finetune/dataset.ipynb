{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a base dataset of reddit conversations, so that it can be filtered and subset in the future.\n",
    "\n",
    "### To Do\n",
    "- Add custom metadata based on analysis of the conversation\n",
    "  - [X] normalized controversiality\n",
    "  - [ ] fix normalized contr., currently some values are greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "# Description: Configuration for the dataset module, could eventually be used as flags\n",
    "HUGGINGFACE_USERNAME = 'brianmatzelle'\n",
    "\n",
    "# change if you know what you're doing\n",
    "RAW_DATA_FILE_NAME = 'posts-11-13-2024'\n",
    "\n",
    "# DONT CHANGE\n",
    "RAW_DATA_FILE = f'data/raw/{RAW_DATA_FILE_NAME}.json'\n",
    "PROCESSED_DATA_FILE = f'data/processed/{RAW_DATA_FILE_NAME}-processed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 21:34:52.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLoaded 54215 rows from data/processed/posts-11-13-2024-processed.json\u001b[0m\n",
      "\u001b[32m2024-11-27 21:34:52.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mConverted json to pandas DataFrame with 54215 rows\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>downs</th>\n",
       "      <th>name</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>is_video</th>\n",
       "      <th>deleted</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1dx1b0z</td>\n",
       "      <td>Destiny</td>\n",
       "      <td></td>\n",
       "      <td>New Vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_1dx1b0z</td>\n",
       "      <td>0.95</td>\n",
       "      <td>121</td>\n",
       "      <td>None</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1720304607</td>\n",
       "      <td>None</td>\n",
       "      <td>TuningsGaming</td>\n",
       "      <td>2</td>\n",
       "      <td>248289</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'id': 'lbyv8mn', 'total_awards_received': 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id subreddit selftext      title  downs        name  upvote_ratio  \\\n",
       "0  1dx1b0z   Destiny           New Vegan      0  t3_1dx1b0z          0.95   \n",
       "\n",
       "   ups removed_by_category link_flair_text  ...  no_follow  created_utc  \\\n",
       "0  121                None        Shitpost  ...      False   1720304607   \n",
       "\n",
       "  author_flair_text         author num_comments  subreddit_subscribers  \\\n",
       "0              None  TuningsGaming            2                 248289   \n",
       "\n",
       "   send_replies is_video deleted  \\\n",
       "0          True    False   False   \n",
       "\n",
       "                                            comments  \n",
       "0  [{'id': 'lbyv8mn', 'total_awards_received': 0,...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "with open(PROCESSED_DATA_FILE, 'r') as f:\n",
    "  data = json.load(f)\n",
    "logger.info(f\"Loaded {len(data)} rows from {PROCESSED_DATA_FILE}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "prefilter_len = len(df)\n",
    "logger.info(f\"Converted json to pandas DataFrame with {prefilter_len} rows\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 21:34:52.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m54k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 21:34:52.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mDataset size (posts): 54k\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['destiny' 'hasan_piker' 'politics' 'vaushv' 'millenials' 'news'\n",
      " 'worldnews' 'economics' 'socialism' 'conservative' 'libertarian'\n",
      " 'neoliberal' 'republican' 'democrats' 'progressive' 'daverubin'\n",
      " 'jordanpeterson' 'samharris' 'joerogan' 'thedavidpakmanshow' 'benshapiro'\n",
      " 'themajorityreport' 'seculartalk']\n"
     ]
    }
   ],
   "source": [
    "# alter columns so they're easier to work with\n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "\n",
    "# Show some values that might be helpful for customizing configuration\n",
    "print(df['subreddit'].unique())\n",
    "\n",
    "from utils import to_k\n",
    "posts_count = to_k(len(df))\n",
    "logger.info(f\"Dataset size (posts): {posts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through posts and create conversations by alternating user/assistant with every comment/reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is straight from hell\n",
    "\n",
    "from typing import Dict, Generator, List, Tuple\n",
    "def Turn(role: str, content: str) -> Dict[str, str]:\n",
    "  return {\n",
    "    'role': role,\n",
    "    'content': content\n",
    "  }\n",
    "\n",
    "def traverse_thread(comment: Dict, c_sum: int, role: str = 'assistant') -> Generator[Tuple[List[Dict[str, str]], int], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively traverse a comment thread and yield each individual thread.\n",
    "    \"\"\"\n",
    "    if role not in {'assistant', 'user'}:\n",
    "        raise ValueError(\"role must be 'assistant' or 'user'\")\n",
    "    \n",
    "    if not comment.get('body'):\n",
    "        return\n",
    "    \n",
    "    if comment['body'] == '[deleted]' or comment['body'] == '[removed]':\n",
    "        return\n",
    "  \n",
    "        \n",
    "    # Start the thread with the current comment\n",
    "    current_thread = [Turn(role, comment['body'])]\n",
    "\n",
    "        # Add controversiality only for the current comment\n",
    "    current_c_sum = c_sum + comment.get('controversiality', 0)\n",
    "    \n",
    "    # If no replies, yield the current thread with current_c_sum\n",
    "    if not comment.get('replies'):\n",
    "        yield current_thread, current_c_sum\n",
    "        return\n",
    "    \n",
    "    # Recurse into replies, but pass current_c_sum instead of c_sum\n",
    "    for reply in comment['replies']:\n",
    "        for sub_thread, sub_c_sum in traverse_thread(reply, current_c_sum, 'user' if role == 'assistant' else 'assistant'):\n",
    "            yield current_thread + sub_thread, sub_c_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- Add custom metadata based on analysis of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(post_row):\n",
    "  return {\n",
    "    \"subreddit\": {\n",
    "      \"name\": post_row.get(\"subreddit\", \"unknown\"),\n",
    "      \"subscribers\": post_row.get(\"subreddit_subscribers\", None),\n",
    "    },\n",
    "    \"post\": {\n",
    "      \"score\": post_row.get(\"score\", None),\n",
    "      \"upvotes\": post_row.get(\"ups\", None),\n",
    "      \"downvotes\": post_row.get(\"downs\", None),\n",
    "      \"upvote_ratio\": post_row.get(\"upvote_ratio\", None),\n",
    "      \"flair\": post_row.get(\"link_flair_text\", None),\n",
    "      \"author\": post_row.get(\"author\", \"unknown\"),\n",
    "      \"suggested_sort\": post_row.get(\"suggested_sort\", None),\n",
    "    },\n",
    "    \"controversiality\": 0,\n",
    "    \"normalized_controversiality\": 0\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 21:35:13.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mExtracted 643627 conversations from 54215 posts\u001b[0m\n",
      "\u001b[32m2024-11-27 21:35:13.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mDeleting dataframe from memory since it hoards resources and is no longer needed\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metadata': {'subreddit': {'name': 'politics', 'subscribers': 8585506},\n",
       "  'post': {'score': 541,\n",
       "   'upvotes': 541,\n",
       "   'downvotes': 0,\n",
       "   'upvote_ratio': 0.97,\n",
       "   'flair': 'Soft Paywall',\n",
       "   'author': 'croato87',\n",
       "   'suggested_sort': None},\n",
       "  'controversiality': 0,\n",
       "  'normalized_controversiality': 0.0},\n",
       " 'conversation': [{'role': 'system',\n",
       "   'content': 'You are a redditor, having a conversation with another redditor.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Trump Loses It Over Devastating Fox News Poll on Kamala Harris'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'I went from feeling doomed for another Trump administration with Biden to feeling excited about a candidate for once, and like the wind is in our sails. \\n\\nTrump does dumb stuff when he’s cornered. Kamala’s campaign is killing it so far. I think this is going to be a trend and the gap will continue to widen. \\n\\nFeels good man. Feels good to be right about wanting Biden to step down. But even I’m surprised at how United dems have been and how well Kamala is rising to the occasion.'},\n",
       "  {'role': 'user',\n",
       "   'content': \"It's because Trump is a criminal bully, and Harris has dealt with assholes before.\\n\\nWe've all seen it before.\\n\\nIt's like Trump shoved her in the playground, and she stood right back up and kicked him in his ding-ding. He knows she won't stay down like the cowards he's surrounded himself with. He knows she'll just keep kicking him. And she'll laugh while doing it.\\n\\nHe'll run away crying.\"}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import is_post_valid\n",
    "from lib.analysis import normalize_controversiality_rating\n",
    "\n",
    "# set to prevent duplicates which can occur if the final comment is deleted or removed\n",
    "conversations = set()\n",
    "for i, post_row in df.iterrows():\n",
    "    valid, reason = is_post_valid(post_row)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = get_metadata(post_row)\n",
    "    \n",
    "    initial_turn = None\n",
    "    first_comment_type = 'user'\n",
    "    # if the post is deleted or removed, use the first comment as the initial turn (user)\n",
    "    if not post_row['selftext'] == '[deleted]' or post_row['selftext'] == '[removed]':\n",
    "        initial_turn = Turn('user', post_row['selftext'] if post_row['selftext'] else post_row['title'])\n",
    "        first_comment_type = 'assistant'\n",
    "\n",
    "    system_turn = [Turn('system', f\"You are a redditor, having a conversation with another redditor.\")]\n",
    "\n",
    "    if initial_turn is not None:\n",
    "        system_turn.append(initial_turn)\n",
    "\n",
    "    for comment in post_row.get('comments', []):\n",
    "        for thread, c_sum in traverse_thread(comment, 0, first_comment_type):\n",
    "            # set controversiality metadata\n",
    "            metadata[\"controversiality\"] = c_sum\n",
    "            metadata[\"normalized_controversiality\"] = normalize_controversiality_rating(sum=c_sum, thread_length=len(thread))\n",
    "            # Serialize thread with metadata\n",
    "            serialized_thread = json.dumps({\n",
    "                \"metadata\": metadata,\n",
    "                \"conversation\": system_turn + thread\n",
    "            })\n",
    "            conversations.add(serialized_thread)\n",
    "            \n",
    "# Deserialize conversations back into Python objects if needed\n",
    "conversations = [json.loads(conv) for conv in conversations]\n",
    "logger.info(f\"Extracted {len(conversations)} conversations from {len(df)} posts\")\n",
    "logger.info(f\"Deleting dataframe from memory since it hoards resources and is no longer needed\")\n",
    "del df\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-27 21:35:13.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m643k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 21:35:13.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mWriting 643k conversations to data/datasets/2024-election-subreddit-threads-643k(1).json...\u001b[0m\n",
      "\u001b[32m2024-11-27 21:35:38.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mConversations saved to data/datasets/2024-election-subreddit-threads-643k(1).json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON file\n",
    "import json\n",
    "\n",
    "name = '2024-election-subreddit-threads'\n",
    "size_str = to_k(len(conversations))\n",
    "from utils import make_dataset_path\n",
    "dataset_path, hf_dataset_name = make_dataset_path(name, size_str)\n",
    "logger.info(f\"Writing {size_str} conversations to {dataset_path}...\")\n",
    "\n",
    "\n",
    "# Create JSON object with metadata and conversation\n",
    "json_obj = []\n",
    "for conversation_data in conversations:\n",
    "    # Each conversation_data should already include metadata and conversation structure\n",
    "    json_obj.append({\n",
    "        \"metadata\": conversation_data.get(\"metadata\", {}),\n",
    "        \"conversations\": conversation_data.get(\"conversation\", [])\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(json_obj, f, indent=2)\n",
    "logger.info(f\"Conversations saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianmatzelle/anaconda3/envs/election/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10727f2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brianmatzelle/anaconda3/envs/election/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# push to huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=dataset_path)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('HF_TOKEN'):\n",
    "  logger.error(\"No Hugging Face token found, not pushing to hub\")\n",
    "else:\n",
    "  dataset.push_to_hub(f\"{HUGGINGFACE_USERNAME}/{hf_dataset_name}\".lower(), token=os.getenv('HF_TOKEN'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (election_env)",
   "language": "python",
   "name": "election_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
