{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a subset of the dataset for finetuning a GPT-style conversational language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SUBREDDITS = ['hasan_piker']\n",
    "HUGGINGFACE_USER = 'brianmatzelle'\n",
    "HUGGINGFACE_BASE_DATASET = '2024-election-subreddit-threads-643k'\n",
    "\n",
    "# Exports\n",
    "HUGGINGFACE_SUBSET_DATASET = f'{HUGGINGFACE_BASE_DATASET.replace(\"subreddit\", \"-\".join(SUBREDDITS))}'\n",
    "\n",
    "\n",
    "# DO NOT EDIT BELOW THIS LINE\n",
    "# remove -643k from the subset dataset name\n",
    "if HUGGINGFACE_SUBSET_DATASET.endswith('-643k'):\n",
    "  HUGGINGFACE_SUBSET_DATASET = HUGGINGFACE_SUBSET_DATASET[:-4]\n",
    "\n",
    "# Print config\n",
    "print(f'SUBREDDITS: {SUBREDDITS}')\n",
    "print(f'HUGGINGFACE_BASE_DATASET: {HUGGINGFACE_BASE_DATASET}')\n",
    "print(f'HUGGINGFACE_SUBSET_DATASET: {HUGGINGFACE_SUBSET_DATASET}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_BASE_DATASET}\", split = \"train\")\n",
    "prefilter_size = len(dataset)\n",
    "\n",
    "from utils import to_k\n",
    "logger.info(f\"Prefilter dataset size: {to_k(prefilter_size)}\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Analysis)\n",
    "Give the user the option to run an analysis on the dataset.\n",
    "\n",
    "Move these next two cells to an analysis function, add more/better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Count the posts in each subreddit\n",
    "# subreddit_counts = Counter(post['metadata']['subreddit']['name'] for post in dataset)\n",
    "# # Sort by the number of posts in descending order\n",
    "# ranked_subreddits = sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Print the ranking\n",
    "# for i, (subreddit, count) in enumerate(ranked_subreddits, start=1):\n",
    "#     for post in dataset:\n",
    "#         if post['metadata']['subreddit']['name'] == subreddit:\n",
    "#             subscribers = post['metadata']['subreddit']['subscribers']\n",
    "#     print(f\"{i}. r/{subreddit}: {count} posts, {subscribers} subscribers\")\n",
    "\n",
    "# # Optionally log the results if needed\n",
    "# logger.info(f\"Subreddit ranking:\\n{ranked_subreddits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show most controversial posts\n",
    "# from collections import defaultdict\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Create a dictionary to store the posts for each subreddit\n",
    "# controversiality = defaultdict(list)\n",
    "# for post in dataset:\n",
    "#     if post['metadata']['controversiality'] < 90:\n",
    "#         continue\n",
    "#     controversiality[post['metadata']['controversiality']] += post\n",
    "\n",
    "\n",
    "# # print the controversiality\n",
    "# pprint(controversiality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Dynamically choose refinement options)\n",
    "Give the user (the one running this notebook) config options after viewing the data, so they can curate a dataset of their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x['metadata']['subreddit']['name'] in SUBREDDITS)\n",
    "logger.info(f\"Filtered {to_k(prefilter_size - len(dataset))} posts from the dataset\")\n",
    "del prefilter_size\n",
    "\n",
    "filtered_size_k = to_k(len(dataset))\n",
    "logger.info(f\"Dataset size: {filtered_size_k} posts\")\n",
    "\n",
    "# Append the new size to the subset dataset name before saving\n",
    "HUGGINGFACE_SUBSET_DATASET += f\"{filtered_size_k}\"\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Give the user option to analyze the dataset again, after removing unwanted subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "# TODO: implement save locally to json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub\n",
    "Before you can run this cell, you need to\n",
    "\n",
    "1. `pip install huggingface_hub`\n",
    "2. `huggingface-cli login`\n",
    "\n",
    "OR\n",
    "\n",
    "In your shell, run\n",
    "1. `export HF_TOKEN=YOUR_WRITE_ACCESS_TOKEN_FROM_HUGGINGFACE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# push curated dataset to huggingface\n",
    "dataset.push_to_hub(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_SUBSET_DATASET}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "nfl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
