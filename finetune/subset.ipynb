{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a subset of the dataset for finetuning a GPT-style conversational language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T02:53:43.776384Z",
     "start_time": "2024-12-06T02:53:43.772717Z"
    }
   },
   "source": [
    "# Config\n",
    "  # ALL_SUBREDDITS = ['destiny' 'hasan_piker' 'politics' 'vaushv' 'millenials' 'news'\n",
    "  # 'worldnews' 'economics' 'socialism' 'conservative' 'libertarian'\n",
    "  # 'neoliberal' 'republican' 'democrats' 'progressive' 'daverubin'\n",
    "  # 'jordanpeterson' 'samharris' 'joerogan' 'thedavidpakmanshow' 'benshapiro'\n",
    "  # 'themajorityreport' 'seculartalk']\n",
    "SUBREDDITS: [str] = ['hasan_piker']\n",
    "# HUGGINGFACE_USER = 'brianmatzelle'\n",
    "HUGGINGFACE_USER: str = 'BinghamtonUniversity'\n",
    "K_COUNT: int = 173\n",
    "HUGGINGFACE_BASE_DATASET: str = f'2024-election-subreddit-threads-{K_COUNT}k'\n",
    "\n",
    "# Exports\n",
    "SUBREDDIT_NAME: str = \"-\".join(SUBREDDITS)\n",
    "HUGGINGFACE_SUBSET_DATASET: str = f'{HUGGINGFACE_BASE_DATASET.replace(\"subreddit\", SUBREDDIT_NAME)}'\n",
    "\n",
    "\n",
    "# DO NOT EDIT BELOW THIS LINE\n",
    "# remove -637k from the subset dataset name\n",
    "if HUGGINGFACE_SUBSET_DATASET.endswith(f'-{K_COUNT}k'):\n",
    "  HUGGINGFACE_SUBSET_DATASET = HUGGINGFACE_SUBSET_DATASET[:-4]\n",
    "\n",
    "# Print config\n",
    "print(f'SUBREDDITS: {SUBREDDITS}')\n",
    "print(f'HUGGINGFACE_BASE_DATASET: {HUGGINGFACE_BASE_DATASET}')\n",
    "print(f'HUGGINGFACE_SUBSET_DATASET: {HUGGINGFACE_SUBSET_DATASET}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDITS: ['hasan_piker']\n",
      "HUGGINGFACE_BASE_DATASET: 2024-election-subreddit-threads-173k\n",
      "HUGGINGFACE_SUBSET_DATASET: 2024-election-hasan_piker-threads-\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T02:53:48.704429Z",
     "start_time": "2024-12-06T02:53:46.435620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset: Dataset = load_dataset(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_BASE_DATASET}\", split = \"train\")\n",
    "# load locally bc im on a place\n",
    "# dataset = load_dataset('json', data_files=f'data/datasets/2024-election-subreddit-threads-{K_COUNT}k.json', split='train')\n",
    "\n",
    "from utils import to_k\n",
    "from loguru import logger\n",
    "logger.info(f\"Prefilter dataset size: {to_k(len(dataset))}\")\n",
    "dataset[0]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niooi/.cache/pypoetry/virtualenvs/election-crawler-26LMlPyt-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001B[32m2024-12-05 21:53:48.700\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m9\u001B[0m - \u001B[1mPrefilter dataset size: 173k\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'The GOP\\'s big, mysterious enemy: \"They\"',\n",
       "   'metadata': {'author': 'Steve____Stifler',\n",
       "    'controversiality': None,\n",
       "    'created_utc': 1721334266,\n",
       "    'downvotes': 0,\n",
       "    'flair': 'News (US)',\n",
       "    'is_submitter': None,\n",
       "    'no_follow': True,\n",
       "    'removed_by_category': 'moderator',\n",
       "    'score': 343,\n",
       "    'suggested_sort': None,\n",
       "    'title': 'The GOP\\'s big, mysterious enemy: \"They\"',\n",
       "    'total_awards_received': 0,\n",
       "    'upvote_ratio': 0.97,\n",
       "    'upvotes': 343},\n",
       "   'role': 'user'},\n",
       "  {'content': '\"Well who\\'s \\'THEY\\'?? ... ... What the hell is an Aluminium Falcon?\"',\n",
       "   'metadata': {'author': 'mad_cheese_hattwe',\n",
       "    'controversiality': 0,\n",
       "    'created_utc': 1721336569,\n",
       "    'downvotes': 0,\n",
       "    'flair': None,\n",
       "    'is_submitter': False,\n",
       "    'no_follow': True,\n",
       "    'removed_by_category': None,\n",
       "    'score': 30,\n",
       "    'suggested_sort': None,\n",
       "    'title': None,\n",
       "    'total_awards_received': 0,\n",
       "    'upvote_ratio': None,\n",
       "    'upvotes': 30},\n",
       "   'role': 'assistant'},\n",
       "  {'content': '\"Oh oh oh, I\\'m sorry, I thought my dark lord of the Sith could protect a small thermal exhaust port that\\'s only two meters wide!\"',\n",
       "   'metadata': {'author': 'jad4400',\n",
       "    'controversiality': 0,\n",
       "    'created_utc': 1721339621,\n",
       "    'downvotes': 0,\n",
       "    'flair': None,\n",
       "    'is_submitter': False,\n",
       "    'no_follow': True,\n",
       "    'removed_by_category': None,\n",
       "    'score': 11,\n",
       "    'suggested_sort': None,\n",
       "    'title': None,\n",
       "    'total_awards_received': 0,\n",
       "    'upvote_ratio': None,\n",
       "    'upvotes': 11},\n",
       "   'role': 'user'},\n",
       "  {'content': '\"...He\\'s _crying!_\"',\n",
       "   'metadata': {'author': 'SharkSymphony',\n",
       "    'controversiality': 0,\n",
       "    'created_utc': 1721341074,\n",
       "    'downvotes': 0,\n",
       "    'flair': None,\n",
       "    'is_submitter': False,\n",
       "    'no_follow': True,\n",
       "    'removed_by_category': None,\n",
       "    'score': 7,\n",
       "    'suggested_sort': None,\n",
       "    'title': None,\n",
       "    'total_awards_received': 0,\n",
       "    'upvote_ratio': None,\n",
       "    'upvotes': 7},\n",
       "   'role': 'assistant'}],\n",
       " 'metadata': {'controversiality': 0,\n",
       "  'normalized_controversiality': 0.0,\n",
       "  'post': {'author': 'Steve____Stifler',\n",
       "   'created_utc': 1721334266,\n",
       "   'downvotes': 0,\n",
       "   'flair': 'News (US)',\n",
       "   'no_follow': True,\n",
       "   'removed_by_category': 'moderator',\n",
       "   'score': 343,\n",
       "   'suggested_sort': None,\n",
       "   'title': 'The GOP\\'s big, mysterious enemy: \"They\"',\n",
       "   'total_awards_received': 0,\n",
       "   'upvote_ratio': 0.97,\n",
       "   'upvotes': 343},\n",
       "  'subreddit': {'name': 'neoliberal', 'subscribers': 177896}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## To Do (Analysis)\n",
    "Give the user the option to run an analysis on the dataset.\n",
    "\n",
    "Move these next two cells to an analysis function, add more/better analysis."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T02:54:44.461343Z",
     "start_time": "2024-12-06T02:54:13.393775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count the posts in each subreddit\n",
    "subreddit_counts: Counter = Counter(post['metadata']['subreddit']['name'] for post in dataset)\n",
    "# Sort by the number of posts in descending order\n",
    "ranked_subreddits: [(str, int)] = sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get subscriber count for each subreddit\n",
    "subs_map: Dict[str, int] = {}\n",
    "for post in dataset:\n",
    "    sr_metadata = post['metadata']['subreddit']\n",
    "    subreddit_name = sr_metadata['name']\n",
    "    if subreddit_name not in subs_map:\n",
    "        subs_map[subreddit_name] = sr_metadata['subscribers']\n",
    "\n",
    "for i, (subreddit, count) in enumerate(ranked_subreddits, start=1):\n",
    "    subs = subs_map.get(subreddit, 0)\n",
    "    print(f\"{i}. r/{subreddit}: {count} posts, {subs} subs\")\n",
    "\n",
    "# Optionally log the results if needed\n",
    "logger.info(f\"Subreddit ranking:\\n{ranked_subreddits}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('politics', 91061), ('destiny', 21991), ('neoliberal', 13620), ('conservative', 8825), ('thedavidpakmanshow', 5210), ('democrats', 5018), ('samharris', 3501), ('vaushv', 3330), ('worldnews', 3270), ('jordanpeterson', 3200), ('libertarian', 2855), ('hasan_piker', 2241), ('news', 1977), ('republican', 1265), ('joerogan', 1187), ('themajorityreport', 1094), ('economics', 1005), ('seculartalk', 859), ('millenials', 846), ('socialism', 824), ('daverubin', 289), ('benshapiro', 218), ('progressive', 27)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-05 21:54:44.459\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m23\u001B[0m - \u001B[1mSubreddit ranking:\n",
      "[('politics', 91061), ('destiny', 21991), ('neoliberal', 13620), ('conservative', 8825), ('thedavidpakmanshow', 5210), ('democrats', 5018), ('samharris', 3501), ('vaushv', 3330), ('worldnews', 3270), ('jordanpeterson', 3200), ('libertarian', 2855), ('hasan_piker', 2241), ('news', 1977), ('republican', 1265), ('joerogan', 1187), ('themajorityreport', 1094), ('economics', 1005), ('seculartalk', 859), ('millenials', 846), ('socialism', 824), ('daverubin', 289), ('benshapiro', 218), ('progressive', 27)]\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. r/politics: 91061 posts, 8667968 subs\n",
      "2. r/destiny: 21991 posts, 250789 subs\n",
      "3. r/neoliberal: 13620 posts, 177896 subs\n",
      "4. r/conservative: 8825 posts, 1128052 subs\n",
      "5. r/thedavidpakmanshow: 5210 posts, 51685 subs\n",
      "6. r/democrats: 5018 posts, 477550 subs\n",
      "7. r/samharris: 3501 posts, 110082 subs\n",
      "8. r/vaushv: 3330 posts, 66254 subs\n",
      "9. r/worldnews: 3270 posts, 40162440 subs\n",
      "10. r/jordanpeterson: 3200 posts, 305909 subs\n",
      "11. r/libertarian: 2855 posts, 503892 subs\n",
      "12. r/hasan_piker: 2241 posts, 143479 subs\n",
      "13. r/news: 1977 posts, 28557108 subs\n",
      "14. r/republican: 1265 posts, 199853 subs\n",
      "15. r/joerogan: 1187 posts, 1316283 subs\n",
      "16. r/themajorityreport: 1094 posts, 73354 subs\n",
      "17. r/economics: 1005 posts, 4577817 subs\n",
      "18. r/seculartalk: 859 posts, 26571 subs\n",
      "19. r/millenials: 846 posts, 103891 subs\n",
      "20. r/socialism: 824 posts, 458927 subs\n",
      "21. r/daverubin: 289 posts, 22017 subs\n",
      "22. r/benshapiro: 218 posts, 57525 subs\n",
      "23. r/progressive: 27 posts, 77881 subs\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show most controversial posts\n",
    "# from collections import defaultdict\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Create a dictionary to store the posts for each subreddit\n",
    "# controversiality = defaultdict(list)\n",
    "# for post in dataset:\n",
    "#     if post['metadata']['controversiality'] < 90:\n",
    "#         continue\n",
    "#     controversiality[post['metadata']['controversiality']] += post\n",
    "\n",
    "\n",
    "# # print the controversiality\n",
    "# pprint(controversiality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Dynamically choose refinement options)\n",
    "Give the user (the one running this notebook) config options after viewing the data, so they can curate a dataset of their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.filter(lambda x: x['metadata']['subreddit']['name'] in SUBREDDITS)\n",
    "logger.info(f\"Filtered {to_k(len(dataset) - len(subset))} posts from the dataset\")\n",
    "del dataset\n",
    "\n",
    "filtered_size_k = to_k(len(subset))\n",
    "logger.info(f\"Dataset size: {filtered_size_k} posts\")\n",
    "\n",
    "# Append the new size to the subset dataset name before saving\n",
    "HUGGINGFACE_SUBSET_DATASET += f\"{filtered_size_k}\"\n",
    "\n",
    "subset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Prepare for OpenAI\n",
    "openai_subset = subset\n",
    "openai_subset = openai_subset.remove_columns([\"metadata\"])\n",
    "openai_subset = openai_subset.rename_column(\"conversations\", \"messages\")\n",
    "# add a new column for weight, make every value 1\n",
    "# openai_subset = openai_subset.add_column(\"weight\", [1] * len(openai_subset))\n",
    "\n",
    "import os\n",
    "os.makedirs(f\"data/subsets/openai/{SUBREDDIT_NAME}\", exist_ok=True)\n",
    "\n",
    "# save complete subset to jsonl file\n",
    "openai_subset.to_json(f\"data/subsets/openai/{SUBREDDIT_NAME}/{HUGGINGFACE_SUBSET_DATASET}-openai.jsonl\", lines=True)\n",
    "\n",
    "# split into train and test\n",
    "train_subset, test_subset = openai_subset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# save train and test subsets to jsonl files\n",
    "train_subset.to_json(f\"data/subsets/openai/{SUBREDDIT_NAME}/{HUGGINGFACE_SUBSET_DATASET}-openai-train.jsonl\", lines=True)\n",
    "test_subset.to_json(f\"data/subsets/openai/{SUBREDDIT_NAME}/{HUGGINGFACE_SUBSET_DATASET}-openai-test.jsonl\", lines=True)\n",
    "\n",
    "# save openai_subset to jsonl file\n",
    "# openai_subset.to_json(f\"data/subsets/{HUGGINGFACE_SUBSET_DATASET}-openai.jsonl\", lines=True)\n",
    "openai_subset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Give the user option to analyze the dataset again, after removing unwanted subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "# subset.to_json(f\"data/subsets/{HUGGINGFACE_SUBSET_DATASET}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub\n",
    "Before you can run this cell, you need to\n",
    "\n",
    "1. `pip install huggingface_hub`\n",
    "2. `huggingface-cli login`\n",
    "\n",
    "OR\n",
    "\n",
    "In your shell, run\n",
    "1. `export HF_TOKEN=YOUR_WRITE_ACCESS_TOKEN_FROM_HUGGINGFACE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# push curated dataset to huggingface\n",
    "subset.push_to_hub(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_SUBSET_DATASET}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
