{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a subset of the dataset for finetuning a GPT-style conversational language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "  # ALL_SUBREDDITS = ['destiny' 'hasan_piker' 'politics' 'vaushv' 'millenials' 'news'\n",
    "  # 'worldnews' 'economics' 'socialism' 'conservative' 'libertarian'\n",
    "  # 'neoliberal' 'republican' 'democrats' 'progressive' 'daverubin'\n",
    "  # 'jordanpeterson' 'samharris' 'joerogan' 'thedavidpakmanshow' 'benshapiro'\n",
    "  # 'themajorityreport' 'seculartalk']\n",
    "SUBREDDITS = ['hasan_piker']\n",
    "# HUGGINGFACE_USER = 'brianmatzelle'\n",
    "HUGGINGFACE_USER = 'BinghamtonUniversity'\n",
    "K_COUNT = 173\n",
    "HUGGINGFACE_BASE_DATASET = f'2024-election-subreddit-threads-{K_COUNT}k'\n",
    "\n",
    "# Exports\n",
    "SUBREDDIT_NAME = \"-\".join(SUBREDDITS)\n",
    "HUGGINGFACE_SUBSET_DATASET = f'{HUGGINGFACE_BASE_DATASET.replace(\"subreddit\", SUBREDDIT_NAME)}'\n",
    "\n",
    "\n",
    "# DO NOT EDIT BELOW THIS LINE\n",
    "# remove -637k from the subset dataset name\n",
    "if HUGGINGFACE_SUBSET_DATASET.endswith(f'-{K_COUNT}k'):\n",
    "  HUGGINGFACE_SUBSET_DATASET = HUGGINGFACE_SUBSET_DATASET[:-4]\n",
    "\n",
    "# Print config\n",
    "print(f'SUBREDDITS: {SUBREDDITS}')\n",
    "print(f'HUGGINGFACE_BASE_DATASET: {HUGGINGFACE_BASE_DATASET}')\n",
    "print(f'HUGGINGFACE_SUBSET_DATASET: {HUGGINGFACE_SUBSET_DATASET}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_BASE_DATASET}\", split = \"train\")\n",
    "# load locally bc im on a place\n",
    "dataset = load_dataset('json', data_files=f'data/datasets/2024-election-subreddit-threads-{K_COUNT}k.json', split='train')\n",
    "\n",
    "from utils import to_k\n",
    "from loguru import logger\n",
    "logger.info(f\"Prefilter dataset size: {to_k(len(dataset))}\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Analysis)\n",
    "Give the user the option to run an analysis on the dataset.\n",
    "\n",
    "Move these next two cells to an analysis function, add more/better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: there has to be a faster way to do this, it takes a few minutes to run\n",
    "# from collections import Counter\n",
    "\n",
    "# # Count the posts in each subreddit\n",
    "# subreddit_counts = Counter(post['metadata']['subreddit']['name'] for post in dataset)\n",
    "# # Sort by the number of posts in descending order\n",
    "# ranked_subreddits = sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Print the ranking\n",
    "# for i, (subreddit, count) in enumerate(ranked_subreddits, start=1):\n",
    "#     for post in dataset:\n",
    "#         if post['metadata']['subreddit']['name'] == subreddit:\n",
    "#             subscribers = post['metadata']['subreddit']['subscribers']\n",
    "#     print(f\"{i}. r/{subreddit}: {count} posts, {subscribers} subscribers\")\n",
    "\n",
    "# # Optionally log the results if needed\n",
    "# logger.info(f\"Subreddit ranking:\\n{ranked_subreddits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show most controversial posts\n",
    "# from collections import defaultdict\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Create a dictionary to store the posts for each subreddit\n",
    "# controversiality = defaultdict(list)\n",
    "# for post in dataset:\n",
    "#     if post['metadata']['controversiality'] < 90:\n",
    "#         continue\n",
    "#     controversiality[post['metadata']['controversiality']] += post\n",
    "\n",
    "\n",
    "# # print the controversiality\n",
    "# pprint(controversiality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Dynamically choose refinement options)\n",
    "Give the user (the one running this notebook) config options after viewing the data, so they can curate a dataset of their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.filter(lambda x: x['metadata']['subreddit']['name'] in SUBREDDITS)\n",
    "logger.info(f\"Filtered {to_k(len(dataset) - len(subset))} posts from the dataset\")\n",
    "del dataset\n",
    "\n",
    "filtered_size_k = to_k(len(subset))\n",
    "logger.info(f\"Dataset size: {filtered_size_k} posts\")\n",
    "\n",
    "# Append the new size to the subset dataset name before saving\n",
    "HUGGINGFACE_SUBSET_DATASET += f\"{filtered_size_k}\"\n",
    "\n",
    "subset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Prepare for OpenAI\n",
    "openai_subset = subset\n",
    "openai_subset = openai_subset.remove_columns([\"metadata\"])\n",
    "openai_subset = openai_subset.rename_column(\"conversations\", \"messages\")\n",
    "# add a new column for weight, make every value 1\n",
    "# openai_subset = openai_subset.add_column(\"weight\", [1] * len(openai_subset))\n",
    "\n",
    "import os\n",
    "os.makedirs(f\"data/subsets/openai/{SUBREDDIT_NAME}\", exist_ok=True)\n",
    "\n",
    "# save complete subset to jsonl file\n",
    "openai_subset.to_json(f\"data/subsets/openai/{SUBREDDIT_NAME}/{HUGGINGFACE_SUBSET_DATASET}-openai.jsonl\", lines=True)\n",
    "\n",
    "# split into train and test\n",
    "train_subset, test_subset = openai_subset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# save train and test subsets to jsonl files\n",
    "train_subset.to_json(f\"data/subsets/openai/{SUBREDDIT_NAME}/{HUGGINGFACE_SUBSET_DATASET}-openai-train.jsonl\", lines=True)\n",
    "test_subset.to_json(f\"data/subsets/openai/{SUBREDDIT_NAME}/{HUGGINGFACE_SUBSET_DATASET}-openai-test.jsonl\", lines=True)\n",
    "\n",
    "# save openai_subset to jsonl file\n",
    "# openai_subset.to_json(f\"data/subsets/{HUGGINGFACE_SUBSET_DATASET}-openai.jsonl\", lines=True)\n",
    "openai_subset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Give the user option to analyze the dataset again, after removing unwanted subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "# subset.to_json(f\"data/subsets/{HUGGINGFACE_SUBSET_DATASET}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub\n",
    "Before you can run this cell, you need to\n",
    "\n",
    "1. `pip install huggingface_hub`\n",
    "2. `huggingface-cli login`\n",
    "\n",
    "OR\n",
    "\n",
    "In your shell, run\n",
    "1. `export HF_TOKEN=YOUR_WRITE_ACCESS_TOKEN_FROM_HUGGINGFACE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# push curated dataset to huggingface\n",
    "subset.push_to_hub(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_SUBSET_DATASET}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (election_env)",
   "language": "python",
   "name": "election_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
