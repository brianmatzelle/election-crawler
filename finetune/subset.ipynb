{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook is used to create a subset of the dataset for finetuning a GPT-style conversational language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDITS: ['hasan_piker']\n",
      "HUGGINGFACE_BASE_DATASET: 2024-election-subreddit-threads-643k\n",
      "HUGGINGFACE_SUBSET_DATASET: 2024-election-hasan_piker-threads-\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "SUBREDDITS = ['hasan_piker']\n",
    "HUGGINGFACE_USER = 'brianmatzelle'\n",
    "HUGGINGFACE_BASE_DATASET = '2024-election-subreddit-threads-643k'\n",
    "\n",
    "# Exports\n",
    "HUGGINGFACE_SUBSET_DATASET = f'{HUGGINGFACE_BASE_DATASET.replace(\"subreddit\", \"-\".join(SUBREDDITS))}'\n",
    "\n",
    "\n",
    "# DO NOT EDIT BELOW THIS LINE\n",
    "# remove -643k from the subset dataset name\n",
    "if HUGGINGFACE_SUBSET_DATASET.endswith('-643k'):\n",
    "  HUGGINGFACE_SUBSET_DATASET = HUGGINGFACE_SUBSET_DATASET[:-4]\n",
    "\n",
    "# Print config\n",
    "print(f'SUBREDDITS: {SUBREDDITS}')\n",
    "print(f'HUGGINGFACE_BASE_DATASET: {HUGGINGFACE_BASE_DATASET}')\n",
    "print(f'HUGGINGFACE_SUBSET_DATASET: {HUGGINGFACE_SUBSET_DATASET}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianmatzelle/anaconda3/envs/election/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 643627 examples [00:12, 51479.08 examples/s]\n",
      "\u001b[32m2024-11-27 20:20:48.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m643k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 20:20:48.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mPrefilter dataset size: 643k\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are a redditor in a political subreddit, having a conversation with another redditor about politics.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'The Old Ranting, Rambling Trump Was Back at the Republican Convention',\n",
       "   'role': 'user'},\n",
       "  {'content': \"It was very bad. He had a chance to put a nail in the democrats' coffin. He had the crowd and everyone watching at home in the palm of his hands as he discussed the shooting incident. From there he turned into the drunk uncle/raving political lunatic that he has always been. Other than his base, Americans are tired of this. Even worse for Trump, his energy is nowhere near where it was 8 years ago. This is a Massive gift to the democrats. I wonder if any of trump's people have the balls to tell him how terrible this speech was. He has no business being the president again.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Spot on. \\n\\nThat speech was a fucking train wreck. And I’m not just saying that because it’s Trump, that was an absolute shit show. \\n\\nMassive gift to the Dems.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Combine this with essentially zero poll movement after the assassination attempt and Biden becoming more likely to drop out each day and everything is subtly swinging left away from fascism.',\n",
       "   'role': 'assistant'}],\n",
       " 'metadata': {'controversiality': 0,\n",
       "  'normalized_controversiality': 0.0,\n",
       "  'post': {'author': 'CrispyMiner',\n",
       "   'downvotes': 0,\n",
       "   'flair': 'Paywall',\n",
       "   'score': 2883,\n",
       "   'suggested_sort': None,\n",
       "   'upvote_ratio': 0.95,\n",
       "   'upvotes': 2883},\n",
       "  'subreddit': {'name': 'politics', 'subscribers': 8663876}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_BASE_DATASET}\", split = \"train\")\n",
    "# load locally bc im on a place\n",
    "dataset = load_dataset('json', data_files='data/datasets/2024-election-subreddit-threads-643k.json', split='train')\n",
    "\n",
    "from utils import to_k\n",
    "from loguru import logger\n",
    "logger.info(f\"Prefilter dataset size: {to_k(len(dataset))}\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Analysis)\n",
    "Give the user the option to run an analysis on the dataset.\n",
    "\n",
    "Move these next two cells to an analysis function, add more/better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Count the posts in each subreddit\n",
    "# subreddit_counts = Counter(post['metadata']['subreddit']['name'] for post in dataset)\n",
    "# # Sort by the number of posts in descending order\n",
    "# ranked_subreddits = sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Print the ranking\n",
    "# for i, (subreddit, count) in enumerate(ranked_subreddits, start=1):\n",
    "#     for post in dataset:\n",
    "#         if post['metadata']['subreddit']['name'] == subreddit:\n",
    "#             subscribers = post['metadata']['subreddit']['subscribers']\n",
    "#     print(f\"{i}. r/{subreddit}: {count} posts, {subscribers} subscribers\")\n",
    "\n",
    "# # Optionally log the results if needed\n",
    "# logger.info(f\"Subreddit ranking:\\n{ranked_subreddits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show most controversial posts\n",
    "# from collections import defaultdict\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Create a dictionary to store the posts for each subreddit\n",
    "# controversiality = defaultdict(list)\n",
    "# for post in dataset:\n",
    "#     if post['metadata']['controversiality'] < 90:\n",
    "#         continue\n",
    "#     controversiality[post['metadata']['controversiality']] += post\n",
    "\n",
    "\n",
    "# # print the controversiality\n",
    "# pprint(controversiality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do (Dynamically choose refinement options)\n",
    "Give the user (the one running this notebook) config options after viewing the data, so they can curate a dataset of their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 643627/643627 [00:11<00:00, 53898.72 examples/s]\n",
      "\u001b[32m2024-11-27 20:21:00.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m632k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 20:21:00.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mFiltered 632k posts from the dataset\u001b[0m\n",
      "\u001b[32m2024-11-27 20:21:00.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mto_k\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m10k rows\u001b[0m\n",
      "\u001b[32m2024-11-27 20:21:00.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mDataset size: 10k posts\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are a redditor in a political subreddit, having a conversation with another redditor about politics.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Dude is so stubborn.', 'role': 'user'},\n",
       "  {'content': 'What do you expect him to say lmao', 'role': 'assistant'}],\n",
       " 'metadata': {'controversiality': 0,\n",
       "  'normalized_controversiality': 0.0,\n",
       "  'post': {'author': 'Candid_Bicycle_6111',\n",
       "   'downvotes': 0,\n",
       "   'flair': None,\n",
       "   'score': 1090,\n",
       "   'suggested_sort': None,\n",
       "   'upvote_ratio': 0.95,\n",
       "   'upvotes': 1090},\n",
       "  'subreddit': {'name': 'hasan_piker', 'subscribers': 139861}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = dataset.filter(lambda x: x['metadata']['subreddit']['name'] in SUBREDDITS)\n",
    "logger.info(f\"Filtered {to_k(len(dataset) - len(subset))} posts from the dataset\")\n",
    "del dataset\n",
    "\n",
    "filtered_size_k = to_k(len(subset))\n",
    "logger.info(f\"Dataset size: {filtered_size_k} posts\")\n",
    "\n",
    "# Append the new size to the subset dataset name before saving\n",
    "HUGGINGFACE_SUBSET_DATASET += f\"{filtered_size_k}\"\n",
    "\n",
    "subset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 101.24ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 190.37ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a redditor in a political subreddit, having a conversation with another redditor about politics.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Dude is so stubborn.', 'role': 'user'},\n",
       "  {'content': 'What do you expect him to say lmao', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL: Prepare for OpenAI\n",
    "openai_subset = subset\n",
    "openai_subset = openai_subset.remove_columns([\"metadata\"])\n",
    "openai_subset = openai_subset.rename_column(\"conversations\", \"messages\")\n",
    "# add a new column for weight, make every value 1\n",
    "# openai_subset = openai_subset.add_column(\"weight\", [1] * len(openai_subset))\n",
    "\n",
    "# split into train and test\n",
    "train_subset, test_subset = openai_subset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# save train and test subsets to jsonl files\n",
    "train_subset.to_json(f\"data/subsets/train/{HUGGINGFACE_SUBSET_DATASET}-openai-train.jsonl\", lines=True)\n",
    "test_subset.to_json(f\"data/subsets/test/{HUGGINGFACE_SUBSET_DATASET}-openai-test.jsonl\", lines=True)\n",
    "\n",
    "# save openai_subset to jsonl file\n",
    "# openai_subset.to_json(f\"data/subsets/{HUGGINGFACE_SUBSET_DATASET}-openai.jsonl\", lines=True)\n",
    "openai_subset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Give the user option to analyze the dataset again, after removing unwanted subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "# subset.to_json(f\"data/subsets/{HUGGINGFACE_SUBSET_DATASET}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub\n",
    "Before you can run this cell, you need to\n",
    "\n",
    "1. `pip install huggingface_hub`\n",
    "2. `huggingface-cli login`\n",
    "\n",
    "OR\n",
    "\n",
    "In your shell, run\n",
    "1. `export HF_TOKEN=YOUR_WRITE_ACCESS_TOKEN_FROM_HUGGINGFACE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# # push curated dataset to huggingface\n",
    "# subset.push_to_hub(f\"{HUGGINGFACE_USER}/{HUGGINGFACE_SUBSET_DATASET}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (election_env)",
   "language": "python",
   "name": "election_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
