{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraped Political Reddit Posts\n",
    "- r/hasan_piker\n",
    "- r/destin\n",
    "- r/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    def __init__(self, data):\n",
    "        self.subreddit = data.get(\"subreddit\", \"\")\n",
    "        self.subreddit_id = data.get(\"subreddit_id\", \"\")\n",
    "        self.title = data.get(\"title\", \"\")\n",
    "        self.selftext = data.get(\"selftext\", \"\")\n",
    "        self.author = data.get(\"author\", \"\")\n",
    "        self.author_flair = data.get(\"author_flair_text\", \"\")\n",
    "        self.score = data.get(\"score\", 0)\n",
    "        self.upvote_ratio = data.get(\"upvote_ratio\", 0.0)\n",
    "        self.num_comments = data.get(\"num_comments\", 0)\n",
    "        self.created_utc = data.get(\"created_utc\", 0)\n",
    "        self.link_flair = data.get(\"link_flair_text\", \"\")\n",
    "        self.url = data.get(\"url\", \"\")\n",
    "        self.total_awards = data.get(\"total_awards_received\", 0)\n",
    "        self.controversiality = data.get(\"controversiality\", 0)\n",
    "        self.num_reports = data.get(\"num_reports\", 0)\n",
    "        self.comments = []\n",
    "\n",
    "    def add_comment(self, comment_data):\n",
    "        self.comments.append(Comment(comment_data))\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"subreddit\": self.subreddit,\n",
    "            \"subreddit_id\": self.subreddit_id,\n",
    "            \"title\": self.title,\n",
    "            \"selftext\": self.selftext,\n",
    "            \"author\": self.author,\n",
    "            \"author_flair\": self.author_flair,\n",
    "            \"score\": self.score,\n",
    "            \"upvote_ratio\": self.upvote_ratio,\n",
    "            \"num_comments\": self.num_comments,\n",
    "            \"created_utc\": self.created_utc,\n",
    "            \"link_flair\": self.link_flair,\n",
    "            \"url\": self.url,\n",
    "            \"total_awards\": self.total_awards,\n",
    "            \"controversiality\": self.controversiality,\n",
    "            \"num_reports\": self.num_reports,\n",
    "            \"comments\": [comment.to_dict() for comment in self.comments]\n",
    "        }\n",
    "\n",
    "class Comment:\n",
    "    def __init__(self, data):\n",
    "        self.author = data.get(\"author\", \"\")\n",
    "        self.author_flair = data.get(\"author_flair_text\", \"\")\n",
    "        self.body = data.get(\"body\", \"\")\n",
    "        self.score = data.get(\"score\", 0)\n",
    "        self.depth = data.get(\"depth\", 0)\n",
    "        self.controversiality = data.get(\"controversiality\", 0)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"author\": self.author,\n",
    "            \"author_flair\": self.author_flair,\n",
    "            \"body\": self.body,\n",
    "            \"score\": self.score,\n",
    "            \"depth\": self.depth,\n",
    "            \"controversiality\": self.controversiality\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to help us clean parse and clean the raw json file (~4gb of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_clean_json(input_file, output_file, chunk_size=1000):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            # Attempt to load the entire content as a single JSON array\n",
    "            json_data = json.load(f)\n",
    "            if isinstance(json_data, list):\n",
    "                process_json_array(json_data, output_file, chunk_size)\n",
    "            else:\n",
    "                raise ValueError(\"Expected a JSON array or line-delimited JSON\")\n",
    "        except json.JSONDecodeError:\n",
    "            f.seek(0)\n",
    "            process_line_by_line(f, output_file, chunk_size)\n",
    "\n",
    "def process_json_array(json_data, output_file, chunk_size):\n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        # Create chunks and show progress\n",
    "        for i in tqdm(range(0, len(json_data), chunk_size), desc=\"Processing JSON array\"):\n",
    "            chunk = json_data[i:i + chunk_size]\n",
    "            for item in chunk:\n",
    "                cleaned_data = clean_post_data(item)\n",
    "                json.dump(cleaned_data, out)\n",
    "                out.write('\\n')\n",
    "\n",
    "def process_line_by_line(f, output_file, chunk_size):\n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        lines = f.readlines()\n",
    "        for i in tqdm(range(0, len(lines), chunk_size), desc=\"Processing JSON lines\"):\n",
    "            chunk = lines[i:i + chunk_size]\n",
    "            for line in chunk:\n",
    "                try:\n",
    "                    json_data = json.loads(line.strip())\n",
    "                    cleaned_data = clean_post_data(json_data)\n",
    "                    json.dump(cleaned_data, out)\n",
    "                    out.write('\\n')\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON line: {line}\")\n",
    "\n",
    "def clean_post_data(json_data):\n",
    "    post_data = json_data.get(\"data\", {})\n",
    "    comments_data = json_data.get(\"comments\", {})\n",
    "\n",
    "    post = Post(post_data)\n",
    "    for _, comment_data in comments_data.items():\n",
    "        post.add_comment(comment_data)\n",
    "\n",
    "    return post.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data and save to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mparse_and_clean_json\u001b[1;34m(input_file, output_file, chunk_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Attempt to load the entire content as a single JSON array\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_data, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./json/input/posts-11-13-2024.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./json/output/cleaned-posts-11-13-2024.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mparse_and_clean_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mparse_and_clean_json\u001b[1;34m(input_file, output_file, chunk_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a JSON array or line-delimited JSON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[0;32m     11\u001b[0m     f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m     process_line_by_line(f, output_file, chunk_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "input_file = './json/input/posts-11-13-2024.json'\n",
    "output_file = './json/output/cleaned-posts-11-13-2024.json'\n",
    "parse_and_clean_json(input_file, output_file, chunk_size=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "election-crawler-mlEYhPe--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
